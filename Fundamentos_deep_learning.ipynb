{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad 1: Fundamentos de deep learning\n",
        "En el presente notebook se muestran algunos conceptos importantes dentro de las redes neuronales y el deep learning.\n",
        "\n",
        "TensorFlow es una biblioteca de programación numérica de código abierto desarrollada por Google. Su nombre se deriva de los \"tensores\", que son arreglos multidimensionales que se utilizan para representar datos.\n",
        "\n",
        "En TensorFlow, los tensores son el objeto principal que se utiliza para realizar operaciones matemáticas. Los tensores son similares a los arreglos de NumPy, pero se optimizan para el procesamiento en GPU."
      ],
      "metadata": {
        "id": "A18KNjM2EVwg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SUIHPR74DESX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ejemplo una multiplicación entre dos tensores:"
      ],
      "metadata": {
        "id": "883J1MtTKXuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)"
      ],
      "metadata": {
        "id": "7F2F9M_ODUS9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.constant([[7, 8], [9, 0], [1, 2]], dtype=tf.int32)"
      ],
      "metadata": {
        "id": "UNoxmvMPE_y_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = tf.matmul(x,y)"
      ],
      "metadata": {
        "id": "0y64SSdGFCCD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.numpy()"
      ],
      "metadata": {
        "id": "JP9AFpkxFDQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cómo se define una red neuronal en TensorFlow?\\\n",
        "En TensorFlow, se define una red neuronal utilizando la clase Sequential y agregando capas con el método add(). Cada capa tiene una función de activación y un número de neuronas que determina su complejidad."
      ],
      "metadata": {
        "id": "8x8t2zT7PPQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_shape=(8,)))\n",
        "model.add(Dense(1))"
      ],
      "metadata": {
        "id": "e66FDzckFkkq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso se tendria una red neuronal con 8 neuronas en la capa de entrada, con una capa oculta de 16 neuronas y una neurona en la capa de salida. \n"
      ],
      "metadata": {
        "id": "0IL_Hq4mPfws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "dRtU_8mHPeCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función de pérdida o función de costo es una función que mide qué tan bien está funcionando el modelo en los datos de entrenamiento, comparando la salida predicha con la salida esperada. Al entrenar un modelo, el objetivo es minimizar el valor que entrega el la funcion de costo mediainte algun algoritmo de optimización\n",
        "\n",
        "Para problemas de regresión es común utilizar función de error cuadrático medio (MSE, por sus siglas en inglés), que mide la diferencia promedio al cuadrado entre la salida predicha por el modelo y la salida esperada.\n",
        "\n",
        "$MSE = (\\frac{1}{n}) * ∑(y_{pred} - y_{real})^2$\n"
      ],
      "metadata": {
        "id": "j2BUf0Z-gLst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que los valores predichos son todos 0\n",
        "y_true = [0, 1, 2, 3, 4, 5]\n",
        "y_pred = [0, 0, 0, 0, 0, 0]\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "print(f'MSE = {mse(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "Hn_tFs1HgE_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que los valores predichos son la mitad del real\n",
        "y_true = [0, 1, 2, 3, 4, 5]\n",
        "y_pred = [0, 0.5, 1, 1.5, 2.0, 2.5]\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "print(f'MSE = {mse(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "Ht45INeRSZJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que los valores predichos son iguales a los reales\n",
        "y_true = [0, 1, 2, 3, 4, 5]\n",
        "y_pred = [0, 1, 2, 3, 4, 5]\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "print(f'MSE = {mse(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "fSPJ6ptKZ5R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento consiste en iterar, ajustando los parámetros (pesos y bias) de forma adecuada hasta llegar a un valor de MSE mínimo "
      ],
      "metadata": {
        "id": "SziSgBSImXcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para problemas de clasiicación no se suele utilizar el MSE cómo funcion de costo, la función de pérdida comúnmente utilizada para problemas de clasificación binaria en redes neuronales es la Entropía Cruzada Binaria (Binary Cross Entropy en inglés), también conocida como BCE.\n",
        "\n",
        "La BCE mide la diferencia entre la probabilidad predicha por la red neuronal para una clase positiva y la probabilidad real. La fórmula matemática de la BCE es la siguiente:\n",
        "\n",
        "$BCE = -\\frac{1}{n} * ∑(y_{real} * log(y_{pred}) + (1 - y_{real}) * log(1 - y_{pred}))$\n",
        "\n",
        "En un problema de clasiicación binaria, el modelo entrega probabilidades entre 0 y 1, el BCE penaliza el error de manera drastica:"
      ],
      "metadata": {
        "id": "0ez5GsA8nh0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que solo hay una muestra y es totalmente erronea\n",
        "y_true = [0.0]\n",
        "y_pred = [1.0]\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "print(f'BCE = {bce(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "zPBSHNu_Z9tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que solo hay una muestra y el valor predicho se acerca al real\n",
        "y_true = [0.0]\n",
        "y_pred = [0.1]\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "print(f'BCE = {bce(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "bPpT6qxip8oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si hay varias muestras:\n",
        "y_true = [0.0, 1.0, 1.0, 0.0]\n",
        "y_pred = [0.1, 0.6, 0.7, 0.2]\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "print(f'BCE = {bce(y_true, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "61aKTEeoqj36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El optimizador es el que se encarga de ajustar los pesos y bias buscando minimizar la funcion de costo, un optimizador muy conocido y sencillo de aplicar es el SGD (Stochastic Gradient Descent). Tiene un hyperparámetro muy importante que es la taza de aprendizaje o learning rate en ingles. La elección del valor puede hacer que el entrenamiento se demore mucho, o peor, que la funcion de costo nunca converja a un valo mínimo.\n",
        "\n",
        "En tensor flow, los pesos y los bias se tratan como variables, supongamos que tenemos un funcion de costo de la forma:\n",
        "\n",
        "$f(x) = \\frac{1}{2}x^2$\n",
        "\n",
        "Y queremos encontrar su mínimo (sabemos que el mínimo es 0) utilizando SGD, en este caso el peso a ajustar sería $x$, por ahora vamos a decir que cada vez que se ajustan los pesos lo vamos a llamar una epoca: \n",
        "\n"
      ],
      "metadata": {
        "id": "v2nmSZXAuPpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el optimizador\n",
        "learning_rate=0.05\n",
        "opt = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
        "var = tf.Variable(1.0)\n",
        "loss = lambda: (var ** 2)/2.0 \n",
        "step_count = []\n",
        "epochs = []\n",
        "for epoch in range(100):\n",
        "  epochs.append(epoch)\n",
        "  opt.minimize(loss, [var]).numpy()\n",
        "  step_count.append(var.numpy()) "
      ],
      "metadata": {
        "id": "qcyGXOu-seXE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, step_count)\n",
        "plt.grid(color='r', linestyle='-', linewidth=1)"
      ],
      "metadata": {
        "id": "-_U451yRw65x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El optimizador Adam es una variante del Descenso de Gradiente Estocástico (SGD) que se utiliza comúnmente en la optimización de redes neuronales. La principal razón por la que se prefiere Adam sobre el SGD es que es más eficiente y eficaz en la convergencia a mínimos locales y la búsqueda de parámetros óptimos."
      ],
      "metadata": {
        "id": "m8zaF_EIXBb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el optimizador\n",
        "learning_rate=0.05\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "var = tf.Variable(1.0)\n",
        "loss = lambda: (var ** 2)/2.0 \n",
        "step_count = []\n",
        "epochs = []\n",
        "for epoch in range(100):\n",
        "  epochs.append(epoch)\n",
        "  opt.minimize(loss, [var])\n",
        "  step_count.append(var.numpy()) "
      ],
      "metadata": {
        "id": "S7lkFK6Fw-7X"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, step_count)\n",
        "plt.grid(color='r', linestyle='-', linewidth=1)"
      ],
      "metadata": {
        "id": "RTYT_S5TV2Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora funciones de activación\n",
        "\n",
        "En una red neuronal, las funciones de activación se aplican a los resultados de las operaciones de la capa anterior y determinan la salida de cada neurona. Estas funciones son no lineales y se utilizan para agregar no linealidad al modelo y permitir que la red neuronal pueda aproximar funciones más complejas.\n",
        "\n",
        "Algunos ejemplos de funciones de activación utilizadas en redes neuronales son la función sigmoide, la función ReLU (Rectified Linear Unit), la función tanh (tangente hiperbólica) y la función softmax. Cada una de estas funciones tiene características diferentes y es útil para diferentes tipos de tareas. Por ejemplo, la función ReLU se utiliza comúnmente en redes neuronales convolucionales para procesamiento de imágenes, mientras que la función softmax se utiliza a menudo en la capa de salida para clasificación de múltiples clases\n",
        "\n",
        "unciones de activación con tf"
      ],
      "metadata": {
        "id": "SwM4yVzFq2UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoide\n",
        "inputs = np.linspace(-20, 20, 100)\n",
        "outputs = tf.keras.activations.sigmoid(inputs)"
      ],
      "metadata": {
        "id": "8W-znh78WQT7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(inputs, outputs)\n",
        "plt.grid(color='r', linestyle='-', linewidth=1)"
      ],
      "metadata": {
        "id": "mZ_Ne36bsa-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para un clasiicador binario, o sea una sola neurona en la capa de salida, normalmente se utiliza en esa última capa la función sigmoid, entrega valores entre 0 y 1"
      ],
      "metadata": {
        "id": "rKsBUqWguZzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# thanh\n",
        "inputs = np.linspace(-20, 20, 100)\n",
        "outputs = tf.keras.activations.tanh(inputs)"
      ],
      "metadata": {
        "id": "FXW1amE8tIEW"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(inputs, outputs)\n",
        "plt.grid(color='r', linestyle='-', linewidth=1)"
      ],
      "metadata": {
        "id": "1Wh0MULSuCfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se suele utilizar en las capas ocultas"
      ],
      "metadata": {
        "id": "ysKK0aA1u4du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relu\n",
        "inputs = np.linspace(-20, 20, 100)\n",
        "outputs = tf.keras.activations.relu(inputs)"
      ],
      "metadata": {
        "id": "7D17cbbLuEIj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(inputs, outputs)\n",
        "plt.grid(color='r', linestyle='-', linewidth=1)"
      ],
      "metadata": {
        "id": "UI_5avNGuMyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que tanh, se suele utilizar en las capas ocultas"
      ],
      "metadata": {
        "id": "bUsn5q-ju19L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax\n",
        "Utilizada comúnmente en la capa de salida de una red neuronal multiclase. La red debe determinar la probabilidad de que una entrada pertenezca a una de varias categorías. La suma de las probabilidaddes de cada clase debe ser 1."
      ],
      "metadata": {
        "id": "deoIRfZhwL7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax\n",
        "caballo = 0.5\n",
        "perro = -0.4\n",
        "gato = 1.72\n",
        "input = tf.constant([[caballo, perro, gato]], dtype=tf.float32)\n",
        "outputs = tf.keras.activations.softmax(input)"
      ],
      "metadata": {
        "id": "Rqv2oKgDuOQG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En keras, al momento de crear una capa, se puede decir que función de activación se va a utilizar para esa capa:"
      ],
      "metadata": {
        "id": "0vnGbsAu3R3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_shape=(8,), activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "ZFMcC8DRxvIN"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambien al momento de construir el modelo, se le dice, que función de costo utilizar, y que optimizador."
      ],
      "metadata": {
        "id": "-V8YimTb4H8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd', loss='mse')"
      ],
      "metadata": {
        "id": "fKAONYmV4X0l"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este momento, tendriamos lista la red neuronal para el entrenamiento"
      ],
      "metadata": {
        "id": "bTJtiI-j43Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiqmZ2e74mpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}